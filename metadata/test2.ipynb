{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import *\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_684408/579303450.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train_2024 = pd.read_csv('/home/toannn/PythonCode/ISIC2024/dataset/data2024/train-metadata.csv')\n"
     ]
    }
   ],
   "source": [
    "df_train_2024 = pd.read_csv('/home/toannn/PythonCode/ISIC2024/dataset/data2024/train-metadata.csv')\n",
    "df_test_2024 = pd.read_csv('/home/toannn/PythonCode/ISIC2024/dataset/data2024/test-metadata.csv')\n",
    "remove_columns = [col for col in df_train_2024.columns if (col not in df_test_2024.columns and col != \"target\")]\n",
    "df_train_2024 = df_train_2024.drop(columns=remove_columns, axis=1)\n",
    "\n",
    "# for isic_id in tqdm(df_train_2024[\"isic_id\"]):\n",
    "#     image_path = f\"./dataset/data2024/image/{isic_id}.jpg\"\n",
    "\n",
    "#     if os.path.exists(image_path):\n",
    "#         df_train_2024.loc[df_train_2024[\"isic_id\"] == isic_id, \"image_path\"] = image_path\n",
    "#     else:\n",
    "#         print(f\"Image {isic_id} not found\")\n",
    "#         break\n",
    "num_cols = [\n",
    "    'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n",
    "    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n",
    "    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n",
    "    'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
    "    'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n",
    "    'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
    "    'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
    "    'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
    "    'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n",
    "]\n",
    "\n",
    "df_train_2024[num_cols] = df_train_2024[num_cols].fillna(df_train_2024[num_cols].median())\n",
    "df_train, new_num_cols, new_cat_cols = feature_engineering(df_train_2024.copy())\n",
    "df_test, _, _ = feature_engineering(df_test_2024.copy())\n",
    "\n",
    "util_cols = ['isic_id', 'patient_id', 'target']\n",
    "util_cols = util_cols + [\"image_path\"] if \"image_path\" in df_train_2024.columns else util_cols\n",
    "num_cols += new_num_cols\n",
    "# anatom_site_general\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"] + new_cat_cols\n",
    "utils_col = [\"target\"]\n",
    "train_cols =  cat_cols + num_cols + util_cols\n",
    "df_train  = df_train[train_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_684408/3713192340.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test_2024\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'target'"
     ]
    }
   ],
   "source": [
    "df_test_2024.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toannn/Documents/toanenv/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "meta_handler = MetaDataClass(df_train)\n",
    "category_encoder = OrdinalEncoder(\n",
    "    categories='auto',\n",
    "    dtype=int,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-2,\n",
    "    encoded_missing_value=-1,\n",
    ")\n",
    "onhot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "cat_encoders = {'category_encoder': category_encoder, 'one_hot_encoder': onhot_encoder}\n",
    "\n",
    "df_train = meta_handler.processCatAttr(df_train, cat_cols, 'train', cat_encoders, 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped color_uniformity column due to inf values\n"
     ]
    }
   ],
   "source": [
    "minmax_scale = MinMaxScaler()\n",
    "num_encoders = {'minmax_scaler': minmax_scale}\n",
    "df_train = meta_handler.processNumAttr(df_train, num_cols, 'train', num_encoders, 'mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = meta_handler.processCatAttr(df_test, cat_cols, 'test')\n",
    "df_test = meta_handler.processNumAttr(df_test, num_cols, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take colums without utils_col\n",
    "meta_features = [col for col in df_train.columns if col not in utils_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, size=12350, n_patient_id= 12350\n",
      "  Train: index=[    0     1     2 ... 66148 66150 66151]\n",
      "         group=['IP_4152176' 'IP_7299252' 'IP_1117889' ... 'IP_3710285' 'IP_9027701'\n",
      " 'IP_9391134']\n",
      "  Test:  index=[    5     6    14 ... 66141 66144 66149]\n",
      "         group=['IP_4334766' 'IP_7312348' 'IP_6215230' ... 'IP_9025934' 'IP_1535132'\n",
      " 'IP_4379891']\n",
      "Fold 1, size=12816, n_patient_id= 12816\n",
      "  Train: index=[    0     1     2 ... 66148 66149 66151]\n",
      "         group=['IP_4152176' 'IP_7299252' 'IP_1117889' ... 'IP_3710285' 'IP_4379891'\n",
      " 'IP_9391134']\n",
      "  Test:  index=[    3     9    22 ... 66142 66143 66150]\n",
      "         group=['IP_1196674' 'IP_3173681' 'IP_9027701' ... 'IP_1249536' 'IP_9577633'\n",
      " 'IP_9027701']\n",
      "Fold 2, size=11671, n_patient_id= 11671\n",
      "  Train: index=[    1     2     3 ... 66149 66150 66151]\n",
      "         group=['IP_7299252' 'IP_1117889' 'IP_1196674' ... 'IP_4379891' 'IP_9027701'\n",
      " 'IP_9391134']\n",
      "  Test:  index=[    0     4    11 ... 66132 66138 66146]\n",
      "         group=['IP_4152176' 'IP_3260480' 'IP_0973797' ... 'IP_9039449' 'IP_4849578'\n",
      " 'IP_8172184']\n",
      "Fold 3, size=13603, n_patient_id= 13603\n",
      "  Train: index=[    0     3     4 ... 66148 66149 66150]\n",
      "         group=['IP_4152176' 'IP_1196674' 'IP_3260480' ... 'IP_3710285' 'IP_4379891'\n",
      " 'IP_9027701']\n",
      "  Test:  index=[    1     2     7 ... 66135 66137 66151]\n",
      "         group=['IP_7299252' 'IP_1117889' 'IP_6157775' ... 'IP_6420568' 'IP_8405898'\n",
      " 'IP_9391134']\n",
      "Fold 4, size=15712, n_patient_id= 15712\n",
      "  Train: index=[    0     1     2 ... 66149 66150 66151]\n",
      "         group=['IP_4152176' 'IP_7299252' 'IP_1117889' ... 'IP_4379891' 'IP_9027701'\n",
      " 'IP_9391134']\n",
      "  Test:  index=[    8    10    12 ... 66145 66147 66148]\n",
      "         group=['IP_7152379' 'IP_1170604' 'IP_0869661' ... 'IP_2224950' 'IP_6760225'\n",
      " 'IP_3710285']\n"
     ]
    }
   ],
   "source": [
    "df_train = meta_handler.split_data(df_train, 5,True,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df_train_y \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m df_test_x \u001b[38;5;241m=\u001b[39m df_test[meta_features]\n\u001b[0;32m----> 4\u001b[0m df_test_y \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/toanenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "df_train_x = df_train[meta_features]\n",
    "df_train_y = df_train['target']\n",
    "df_test_x = df_test[meta_features]\n",
    "df_test_y = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a simple neural network model to fit the df_train, and training it\n",
    "\n",
    "class MetadataNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(MetadataNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(batch_size, seq_len, -1)\n",
    "        return out, hidden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SUBSAMPLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m N_SPLITS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m gkf \u001b[38;5;241m=\u001b[39m StratifiedGroupKFold(n_splits\u001b[38;5;241m=\u001b[39mN_SPLITS, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mSUBSAMPLE\u001b[49m:\n\u001b[1;32m      5\u001b[0m     df_pos \u001b[38;5;241m=\u001b[39m df_train[df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m     df_neg \u001b[38;5;241m=\u001b[39m df_train[df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SUBSAMPLE' is not defined"
     ]
    }
   ],
   "source": [
    "N_SPLITS = 5\n",
    "gkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "if SUBSAMPLE:\n",
    "    df_pos = df_train[df_train[\"target\"] == 1]\n",
    "    df_neg = df_train[df_train[\"target\"] == 0]\n",
    "    df_neg = df_neg.sample(frac=SUBSAMPLE_RATIO, random_state=42)\n",
    "    df_train = pd.concat([df_pos, df_neg]).sample(frac=1.0, random_state=42).reset_index(drop=True)    \n",
    "\n",
    "df_train[\"fold\"] = -1\n",
    "for idx, (train_idx, val_idx) in enumerate(gkf.split(df_train, df_train[\"target\"], groups=df_train[\"patient_id\"])):\n",
    "    df_train.loc[val_idx, \"fold\"] = idx\n",
    "\n",
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "def custom_lgbm_metric(y_true, y_hat):\n",
    "    # TODO: Refactor with the above.\n",
    "    min_tpr = 0.80\n",
    "    v_gt = abs(y_true-1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return \"pauc80\", partial_auc, True\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        # \"metric\": \"custom\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"device\": \"gpu\"\n",
    "    }\n",
    "    scores = []\n",
    "    for fold in range(N_SPLITS):\n",
    "        _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "        _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "        dtrain = lgb.Dataset(_df_train[train_cols], label=_df_train[\"target\"])\n",
    "        gbm = lgb.train(param, dtrain)\n",
    "        preds = gbm.predict(_df_valid[train_cols])\n",
    "        score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "if OPTIMIZE_OPTUNA:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# lgb_params = {\n",
    "#     'objective': 'binary',\n",
    "#     # \"random_state\": 42,\n",
    "#     \"n_estimators\": 1500,\n",
    "#     'learning_rate': 0.001,\n",
    "#     'bagging_freq': 1,\n",
    "#     'pos_bagging_fraction': 0.75,\n",
    "#     'neg_bagging_fraction': 0.05,\n",
    "#     'feature_fraction': 0.6,\n",
    "#     'lambda_l1': 0.2,\n",
    "#     'lambda_l2': 0.7,\n",
    "#     'num_leaves': 35,\n",
    "#     \"min_data_in_leaf\": 50,\n",
    "#     \"verbosity\": -1,\n",
    "#     \"device\": \"gpu\"\n",
    "#     # \"extra_trees\": True\n",
    "# }\n",
    "# new_params =  {\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"verbosity\": -1,\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"n_estimators\": 2000,\n",
    "#     'learning_rate': 0.03,    \n",
    "#     'lambda_l1': 0.0004681884533249742, \n",
    "#     'lambda_l2': 8.765240856362274, \n",
    "#     'num_leaves': 136, \n",
    "#     'feature_fraction': 0.5392005444882538, \n",
    "#     'bagging_fraction': 0.9577412548866563, \n",
    "#     'bagging_freq': 6,\n",
    "#     'min_child_samples': 60,\n",
    "#     \"device\": \"cpu\"\n",
    "# }\n",
    "new_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"n_estimators\": 200,\n",
    "    'learning_rate': 0.05,    \n",
    "    'lambda_l1': 0.0004681884533249742, \n",
    "    'lambda_l2': 8.765240856362274, \n",
    "    'num_leaves': 136, \n",
    "    'feature_fraction': 0.5392005444882538, \n",
    "    'bagging_fraction': 0.9577412548866563, \n",
    "    'bagging_freq': 6,\n",
    "    'min_child_samples': 60,\n",
    "    \"device\": \"gpu\"\n",
    "}\n",
    "lgb_scores = []\n",
    "lgb_models = []\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(N_SPLITS):\n",
    "    _df_train = df_train[df_train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    _df_valid = df_train[df_train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    # model = lgb.LGBMClassifier(**new_params)\n",
    "    model = VotingClassifier([(f\"lgb_{i}\", lgb.LGBMClassifier(random_state=i, **new_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(_df_train[train_cols], _df_train[\"target\"])\n",
    "    preds = model.predict_proba(_df_valid[train_cols])[:, 1]\n",
    "    score = comp_score(_df_valid[[\"target\"]], pd.DataFrame(preds, columns=[\"prediction\"]), \"\")\n",
    "    print(f\"fold: {fold} - Partial AUC Score: {score:.5f}\")\n",
    "    lgb_models.append(model)\n",
    "    oof_single = _df_valid[[\"isic_id\", \"target\"]].copy()\n",
    "    oof_single[\"pred\"] = preds\n",
    "    oof_df = pd.concat([oof_df, oof_single])\n",
    "\n",
    "lgbm_score = comp_score(oof_df[\"target\"], oof_df[\"pred\"], \"\")\n",
    "print(f\"LGBM Score: {lgbm_score:.5f}\")\n",
    "\n",
    "if DISPLAY_FEATURE_IMPORTANCE:\n",
    "    # Make sure that this is a single model, not voting classifier. Will handle that later on.\n",
    "    importances = np.mean([model.feature_importances_ for model in lgb_models], 0)\n",
    "    df_imp = pd.DataFrame({\"feature\": model.feature_name_, \"importance\": importances}).sort_values(\"importance\").reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.barh(df_imp[\"feature\"], df_imp[\"importance\"])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toanenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
